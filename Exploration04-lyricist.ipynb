{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "forced-malta",
   "metadata": {},
   "source": [
    "# 프로젝트 : 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "running-eagle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['[Verse 1]', 'They come from everywhere', 'A longing to be free', 'They come to join us here', 'From sea to shining sea And they all have a dream', 'As people always will', 'To be safe and warm', 'In that shining city on the hill Some wanna slam the door', 'Instead of opening the gate', \"Aw, let's turn this thing around\", 'Before it gets too late [Chorus]', \"It's up to me and you\", 'Love can conquer hate', 'I know this to be true', \"That's what makes us great [Verse 2]\"]\n"
     ]
    }
   ],
   "source": [
    "#데이터 읽어오기\n",
    "import glob\n",
    "import os, re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "#여러개의 txt파일을 모두 읽어서 raw_corpus에 담는다\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\")as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "        \n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-newman",
   "metadata": {},
   "source": [
    "# 데이터 정제하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rational-detection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "# 입력된 문장을\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "#     2. 특수문자 양쪽에 공백을 넣고\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "#     5. 다시 양쪽 공백을 지웁니다\n",
    "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() #1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1\", sentence)#2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) #3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\",\" \", sentence) #4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' #6\n",
    "    return sentence\n",
    "\n",
    "# 이 문장이 어떻게 필터링되는지 확인\n",
    "print(preprocess_sentence(\"This @_is ;;;sample     sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adjustable-sapphire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150691"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#정제된 문장을 모으는 곳\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    #우리가 원하지 않는 문장은 건너뛸 수 있게\n",
    "    if len(sentence) == 0: continue  #길이가 0인 문장은 건너뜀\n",
    "    if sentence[-1] == \":\": continue \n",
    "    if sentence[0] == \"[\": continue # [verse]같은 건 건너뜀\n",
    "    if sentence.count(\" \") > 10 : continue    \n",
    "        \n",
    "    #정제를 하고 담아주기\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "    \n",
    "#정제된 결과를 10개만 확인\n",
    "corpus[:10]\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "basic-listening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   42   64 ...    0    0    0]\n",
      " [   2    9 2964 ...    0    0    0]\n",
      " [   2   42   64 ...    0    0    0]\n",
      " ...\n",
      " [   2  554   19 ...    0    0    0]\n",
      " [   2  121   36 ...    0    0    0]\n",
      " [   2    5   23 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fc149f77f50>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "   \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, # 단어장의 크기설정 (12000이상 권장함)\n",
    "        filters=' ', #이미 문장을 정제해서 filters필요x\n",
    "        oov_token=\"<unk>\" # 사전에 없는 단어는 <unk>로 \n",
    "    )\n",
    "    \n",
    "    #corpus를 이용해 tokenizer 내부의 단어장을 완성하는 함수\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    #준비한 tokenizer를 이용해서 corpus를 tensor로 변환\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줌(padding)\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줌.\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용함\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "relevant-vertex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   42   64   73  704    3    0    0    0    0    0    0    0]\n",
      " [   2    9 2964   10   27  262    3    0    0    0    0    0    0]\n",
      " [   2   42   64   10 2126  126   92    3    0    0    0    0    0]\n",
      " [   2   73  542   10 1068  542    8   42   25   76    9  344    3]\n",
      " [   2   82  173  171   84    3    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "#토큰화한 tensor\n",
    "print(tensor[:5, :13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "equipped-hands",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "#문장이 2로 시작하는데 알아보기 위해서 단어사전이 어떻게 구축되어있는지 아래코드로 확인\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "    \n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-person",
   "metadata": {},
   "source": [
    "#### 왜 시작이 2고 마지막이 3으로 끝나는지 알 수 있었음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-bronze",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-immunology",
   "metadata": {},
   "source": [
    "## 데이터 셋 분리하기     \n",
    "corpus 내의 첫 번째 문장에 대해 생성된 소스와 타겟 문장을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "superb-bunch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  42  64  73 704   3   0   0   0   0   0   0   0   0]\n",
      "[ 42  64  73 704   3   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성하기\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높음\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성하기\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "increasing-trinity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (120552, 14)\n",
      "Target Train: (120552, 14)\n"
     ]
    }
   ],
   "source": [
    "#train_test_split()함수를 사용해서 훈련데이터와 평가데이터 분리하기!\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#총 데이터의 20%를 평가 데이터셋으로 사용\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-poster",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "neutral-safety",
   "metadata": {},
   "source": [
    "## 데이터 셋 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acoustic-million",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 12,000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12,001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만들기\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-possession",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "handled-intellectual",
   "metadata": {},
   "source": [
    "## 인공지능 만들기\n",
    "\n",
    "* 모델의 Embedding Size와 Hidden Size를 조절해서 10Epoch안에     \n",
    "val_loss값을 2.2 수준으로 줄일 수 있는 모델 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "tracked-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "important-creature",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[-9.29317102e-05, -1.00671590e-04, -7.54281136e-05, ...,\n",
       "          2.08734346e-04,  1.46751379e-04,  1.87139274e-04],\n",
       "        [-2.21639813e-04, -1.47041326e-04, -2.22804549e-04, ...,\n",
       "          5.61711262e-04,  1.88144055e-04,  3.50184855e-04],\n",
       "        [-4.97532950e-04, -1.06582105e-04, -3.02208296e-04, ...,\n",
       "          2.92190496e-04,  2.71699188e-04,  5.50822238e-04],\n",
       "        ...,\n",
       "        [ 6.32112438e-04,  1.05727429e-03, -4.74247965e-04, ...,\n",
       "         -8.66326562e-04, -1.16343761e-03,  9.99487820e-04],\n",
       "        [ 7.24294572e-04,  1.00535958e-03, -2.16201792e-04, ...,\n",
       "         -9.82033089e-04, -1.12775527e-03,  9.26082139e-04],\n",
       "        [ 7.25767633e-04,  7.76509114e-04,  8.83804751e-05, ...,\n",
       "         -1.05894927e-03, -9.79892211e-04,  6.26949186e-04]],\n",
       "\n",
       "       [[-9.29317102e-05, -1.00671590e-04, -7.54281136e-05, ...,\n",
       "          2.08734346e-04,  1.46751379e-04,  1.87139274e-04],\n",
       "        [-1.96463152e-04, -1.36801667e-04,  1.67046994e-04, ...,\n",
       "          6.00531173e-04,  1.36899500e-04,  2.86312454e-04],\n",
       "        [-1.53008150e-04, -3.90954046e-05,  1.49171334e-04, ...,\n",
       "          5.99377148e-04, -4.42288219e-05,  1.20735065e-04],\n",
       "        ...,\n",
       "        [ 6.16340782e-04,  1.07690867e-03,  3.73979128e-04, ...,\n",
       "          4.89837315e-04, -1.33238954e-03, -8.87902919e-04],\n",
       "        [ 5.32926817e-04,  1.27467629e-03,  4.41557320e-04, ...,\n",
       "          4.80316638e-04, -1.53376570e-03, -7.59261718e-04],\n",
       "        [ 4.08376014e-04,  1.19183678e-03,  5.85925125e-04, ...,\n",
       "          3.58435325e-04, -1.49724830e-03, -7.66698620e-04]],\n",
       "\n",
       "       [[-9.29317102e-05, -1.00671590e-04, -7.54281136e-05, ...,\n",
       "          2.08734346e-04,  1.46751379e-04,  1.87139274e-04],\n",
       "        [-2.90048483e-04,  9.36970682e-05, -1.39341253e-04, ...,\n",
       "          3.07856011e-04,  6.83293096e-04,  1.56174108e-04],\n",
       "        [-1.70323663e-04,  4.64964920e-04, -9.78051976e-05, ...,\n",
       "          2.50533805e-04,  1.01359596e-03,  1.23207472e-04],\n",
       "        ...,\n",
       "        [-6.76482086e-05, -1.34311314e-03,  1.82917831e-03, ...,\n",
       "         -2.75248312e-04,  4.98058682e-04, -2.62971735e-03],\n",
       "        [-7.39551833e-05, -1.57304108e-03,  1.98269007e-03, ...,\n",
       "         -2.64113100e-04,  4.46967228e-04, -2.92786886e-03],\n",
       "        [-7.62188283e-05, -1.76547805e-03,  2.12587812e-03, ...,\n",
       "         -2.54320679e-04,  4.07947024e-04, -3.16826208e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 6.86277999e-05,  3.19974351e-04, -1.80459087e-04, ...,\n",
       "         -7.43631390e-05,  3.24510802e-05, -1.44067541e-04],\n",
       "        [-4.48981518e-05,  5.62521629e-04, -2.53215898e-04, ...,\n",
       "         -5.44338138e-04,  1.57165894e-04, -5.06911092e-05],\n",
       "        [-1.63679520e-04,  6.28107111e-04, -3.03286710e-04, ...,\n",
       "         -7.38140254e-04,  3.80299811e-04,  1.45076279e-04],\n",
       "        ...,\n",
       "        [ 5.76744496e-04,  7.01268727e-04, -1.35436677e-03, ...,\n",
       "         -1.56166701e-04,  7.54145905e-04,  3.67159868e-04],\n",
       "        [ 5.51332894e-04,  6.06435817e-04, -1.21570565e-03, ...,\n",
       "         -2.18509267e-05,  4.61054675e-04,  3.40797793e-04],\n",
       "        [ 4.76052053e-04,  4.85629571e-04, -1.14584353e-03, ...,\n",
       "          2.34656659e-06,  7.47030208e-05,  3.41005129e-04]],\n",
       "\n",
       "       [[-9.29317102e-05, -1.00671590e-04, -7.54281136e-05, ...,\n",
       "          2.08734346e-04,  1.46751379e-04,  1.87139274e-04],\n",
       "        [-9.74548748e-05, -1.66186088e-04, -1.61025906e-04, ...,\n",
       "          5.06279408e-04,  6.57203782e-04,  2.22678835e-04],\n",
       "        [-7.50485779e-05, -3.73398623e-04, -2.35129832e-04, ...,\n",
       "          9.95922368e-04,  8.47513438e-04, -3.27426060e-05],\n",
       "        ...,\n",
       "        [ 4.59805276e-04, -1.67309961e-04,  7.68348167e-04, ...,\n",
       "          4.42978519e-04,  7.14718481e-04, -1.00076920e-03],\n",
       "        [ 3.41491745e-04, -4.57709248e-04,  9.38213954e-04, ...,\n",
       "          3.00737185e-04,  6.73697272e-04, -1.36792450e-03],\n",
       "        [ 2.31546437e-04, -7.52085703e-04,  1.10455323e-03, ...,\n",
       "          1.91880477e-04,  6.15768309e-04, -1.76092540e-03]],\n",
       "\n",
       "       [[-9.29317102e-05, -1.00671590e-04, -7.54281136e-05, ...,\n",
       "          2.08734346e-04,  1.46751379e-04,  1.87139274e-04],\n",
       "        [-4.15869581e-05, -3.60731821e-04,  3.61095990e-05, ...,\n",
       "          1.16583469e-04,  4.60384152e-04,  3.01041815e-04],\n",
       "        [-2.78848020e-04, -3.64139734e-04, -5.43932147e-05, ...,\n",
       "          6.79884324e-05,  4.41432087e-04,  1.45377024e-04],\n",
       "        ...,\n",
       "        [-6.49025780e-04, -4.60296840e-04,  8.46762850e-04, ...,\n",
       "         -2.47967255e-04,  5.35408268e-04, -1.18199550e-03],\n",
       "        [-6.07140770e-04, -7.36767077e-04,  1.10142713e-03, ...,\n",
       "         -2.53787905e-04,  4.71205305e-04, -1.68299268e-03],\n",
       "        [-5.64625545e-04, -1.00008864e-03,  1.33351178e-03, ...,\n",
       "         -2.53713108e-04,  4.13496018e-04, -2.14201305e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 셋에서 데이터 한 배치만 불러오기\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣어보기\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "russian-kansas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "married-melissa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "588/588 [==============================] - 375s 634ms/step - loss: 3.8920\n",
      "Epoch 2/10\n",
      "588/588 [==============================] - 372s 633ms/step - loss: 2.9609\n",
      "Epoch 3/10\n",
      "588/588 [==============================] - 373s 633ms/step - loss: 2.7775\n",
      "Epoch 4/10\n",
      "588/588 [==============================] - 374s 635ms/step - loss: 2.6478\n",
      "Epoch 5/10\n",
      "588/588 [==============================] - 374s 636ms/step - loss: 2.5251\n",
      "Epoch 6/10\n",
      "588/588 [==============================] - 373s 635ms/step - loss: 2.4260\n",
      "Epoch 7/10\n",
      "588/588 [==============================] - 374s 636ms/step - loss: 2.3344\n",
      "Epoch 8/10\n",
      "588/588 [==============================] - 374s 636ms/step - loss: 2.2487\n",
      "Epoch 9/10\n",
      "588/588 [==============================] - 373s 635ms/step - loss: 2.1653\n",
      "Epoch 10/10\n",
      "588/588 [==============================] - 373s 634ms/step - loss: 2.0898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc1495de490>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "generous-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장 생성 함수\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듦\n",
    "    #    1. 입력받은 문장의 텐서를 입력하기\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냄\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙임\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성 마침\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "twelve-donna",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "outdoor-sheffield",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i know you re smiling down on me <end> '"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i know\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "french-nicaragua",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> if we re not there <end> '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> if we\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-champagne",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
